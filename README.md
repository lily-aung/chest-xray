# ðŸ©» Chest X-Ray Disease Classification

This repository implements an end-to-end medical imaging pipeline for Chest X-ray multi-class classification (Normal / Pneumonia / Tuberculosis), covering:
- Data exploration and preprocessing
- Classical ML baselines (HOG + ML)
- Deep learning models (CNNs, Backbone + Linear Probe : ResNet , DenseNet, EfficientNet, Swin Transformer)
- Model selection and explainability (Grad-CAM)
- Deployment via Docker (CLI inference + FastAPI service)
- Lightweight web UI for visualization
The project is designed to be reproducible, modular, and deployment-ready.


# âš¡ Quick Demo 

URL: http://3.96.47.158 <- Temporary disable the link
```text 
username: labviewer
password: labpassword
```


# ðŸ”¬ Project Workflow (High Level)

```text
Raw Data
  â†“
EDA & Preprocessing
  â†“
Model Training (CNN / Transformer / HOG)
  â†“
Evaluation & Error Analysis
  â†“
Best Model Selection
  â†“
Model Bundle Creation
  â†“
Docker Inference / API
  â†“
Web UI + Grad-CAM Visualization
```

## Repository Structure
```text
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ environment.yml        <- Conda environment for reproducibility
â”œâ”€â”€ pyproject.toml

â”œâ”€â”€ artifacts/              Classical ML & auxiliary model artifacts
â”œâ”€â”€ bundles/                Deployment-ready model bundles
â”œâ”€â”€ configs/               <- YAML configs for experiments and models
â”‚   â”œâ”€â”€ base.yaml
â”‚   â”œâ”€â”€ cnn.yaml
â”‚   â”œâ”€â”€ resnet50.yaml
â”‚   â”œâ”€â”€ densenet121.yaml
â”‚   â”œâ”€â”€ efficientnet_b0.yaml
â”‚   â””â”€â”€ swin_tiny_patch4_window7_224.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               <- Original X-ray images (not tracked in Git)
â”‚   â”œâ”€â”€ eda/               <- Dataset statistics and plots
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ val.csv
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ dist/                  <- Exported Docker images (.tar.gz)
â”œâ”€â”€ docker/                <- Dockerfiles and build context
â”œâ”€â”€ docs/                  <- Guides for Setup & WorkFlow 
â”œâ”€â”€ logs/                  <- Training and runtime logs
â”œâ”€â”€ notebooks/             <- Exploratory data analysis and preprocessing
â”‚   â”œâ”€â”€ 01.EDA.ipynb
â”‚   â”‚   â”œâ”€â”€ Dataset statistics and class distribution
â”‚   â”‚   â”œâ”€â”€ Image resolution and aspect ratio analysis
â”‚   â”‚   â””â”€â”€ Identification of preprocessing issues
â”‚   â”‚
â”‚   â”œâ”€â”€ 02.Preprocessing_Blackborder.ipynb
â”‚   â”‚   â”œâ”€â”€ Detection and removal of black borders
â”‚   â”‚   â”œâ”€â”€ Visualization of preprocessing effects
â”‚   â”‚   â””â”€â”€ Impact on image geometry
â”‚   â”‚
â”‚   â”œâ”€â”€ 03.EDA_FinalizePreprocessing.ipynb
â”‚   â”‚   â”œâ”€â”€ Post-preprocessing validation
â”‚   â”‚   â”œâ”€â”€ Final dataset statistics
â”‚   â”‚   â””â”€â”€ Sanity checks before model training
â”‚   â”‚
â”‚   â”œâ”€â”€ aspect_ratio_distribution.png
â”‚   â”œâ”€â”€ image_resolution_distribution.png
â”‚   â”œâ”€â”€ test_connection.py      <- MLflow / W&B connectivity test
â”‚   â”‚
â”‚   â”œâ”€â”€ mlflow.db               <- Local MLflow tracking (not committed)
â”‚   â””â”€â”€ mlruns/                 <- MLflow runs (not committed)
â”œâ”€â”€ reports/                <-  Results, plots, inference outputs
â”‚   â”œâ”€â”€ figures/           <- Confusion matrices, ROC, Grad-CAM
â”‚   â””â”€â”€ best_model_eval/   <- Final evaluation outputs
â”œâ”€â”€ references/            <- Background material and references
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/              <- Dataset and DataLoader logic
â”‚   â”œâ”€â”€ models/            <- Model builders (CNNs)
â”‚   â”œâ”€â”€ engine/            <- Trainer: Training, evaluation, callbacks
â”‚   â”œâ”€â”€ preprocessing/     <- Image preprocessing (e.g., CLAHE)
â”‚   â”œâ”€â”€ utils/             <- Logging, metrics, reproducibility
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ train_CNN.py
â”‚   â”œâ”€â”€ train_LinearProbe.py
â”‚   â”œâ”€â”€ train_hog.py
â”‚   â””â”€â”€ eval_best_models.py
â””â”€â”€ wandb                   <- Expirementtracking artifacts
```
I have also created a README for bundles, docker, src, data and reports


## Models Implemented
1. Deeplerning :CNN
2. Linear Probe + Fine-tuned : Deeplearning (ResNet-50, DenseNet-121, EfficientNet-B0, Swin-Tiny )
3. HOG + MLP / XGBoost / RandomForest baseline
4. Explainability (Grad-CAM / attention)

## Model Selection and Evaluation

We systematically select the best-performing models from multiple training paradigms and evaluate them on a shared held-out test set, while preserving each modelâ€™s native preprocessing pipeline. Final outputs are deployment-ready artifacts, including predictions, evaluation metrics, calibration analysis, and misclassification diagnostics.

The evaluated model families include:
- **CNN baselines** (end-to-end deep learning)
- **Linear Probe models** (frozen backbone with linear classifier)
- **HOG-based classical models** (MLP, Random Forest, XGBoost)

Model selection is driven entirely by **MLflow experiment tracking**, without manual configuration or post-hoc tuning, ensuring a fully reproducible and unbiased evaluation process.

## Explainability
Only support for CNN at this stage 
- Grad-CAM
- Grad-CAM++
- Smooth Grad-CAM

## Deployment
- CLI inference container
- FastAPI prediction service
- Self-contained model bundles
- Web UI (image upload â†’ prediction â†’ Grad-CAM)

<p align="left">
  <strong>Inference Workflow</strong><br>
  <img src="reports/figures/workflow-guide/InferenceWorkFlow.png" width="50%"/>
</p>

## Reproducibility
- YAML-based experiment configs
- MLflow tracking (local)
- Deterministic seeds
- Versioned model bundles

## ðŸ“˜ Documentation

- [Setup Guide (Training Environment)](docs/setup-guide.md)
- [Workflow Guide (Training, Evaluation, Makefile)](docs/workflow-guide.md)
- [Technical Test: Chest X-Ray Classification](docs/TechnicalReport.pdf)

## License
See LICENSE for details.

## Disclaimer
This project is for research and educational purposes only.
It is not intended for clinical use, diagnosis, or medical decision-making.
